<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>The Weak Law of Large Numbers</title>
  <meta name="description" content="The Weak Law of Large Numbers" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="The Weak Law of Large Numbers" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="The Weak Law of Large Numbers" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  


<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path=""><a href="#weak-law-of-large-numbers"><i class="fa fa-check"></i><b>1</b> Weak Law of Large Numbers</a>
<ul>
<li class="chapter" data-level="1.1" data-path=""><a href="#set-up-the-space"><i class="fa fa-check"></i><b>1.1</b> Set Up the Space</a></li>
<li class="chapter" data-level="1.2" data-path=""><a href="#coin-tosses"><i class="fa fa-check"></i><b>1.2</b> Coin Tosses</a></li>
<li class="chapter" data-level="1.3" data-path=""><a href="#tossing-more-coins"><i class="fa fa-check"></i><b>1.3</b> Tossing more coins</a></li>
<li class="chapter" data-level="1.4" data-path=""><a href="#plot-distinct-distributions"><i class="fa fa-check"></i><b>1.4</b> Plot distinct distributions</a></li>
<li class="chapter" data-level="1.5" data-path=""><a href="#weak-law-of-large-numbers-1"><i class="fa fa-check"></i><b>1.5</b> Weak Law of Large Numbers</a></li>
<li class="chapter" data-level="1.6" data-path=""><a href="#the-wlln-is-about-a-probability-limit."><i class="fa fa-check"></i><b>1.6</b> The WLLN is about a probability limit.</a></li>
<li class="chapter" data-level="1.7" data-path=""><a href="#chebyshevs-inequality"><i class="fa fa-check"></i><b>1.7</b> Chebyshev’s Inequality</a></li>
<li class="chapter" data-level="1.8" data-path=""><a href="#chevyshev-inequlaity-for-the-sample-average"><i class="fa fa-check"></i><b>1.8</b> Chevyshev Inequlaity for the Sample Average</a></li>
<li class="chapter" data-level="1.9" data-path=""><a href="#using-chebyshevs-inequality-for-the-sample-mean"><i class="fa fa-check"></i><b>1.9</b> Using Chebyshev’s Inequality for the Sample Mean</a></li>
<li class="chapter" data-level="1.10" data-path=""><a href="#case-1-variance-known-sample-size-known"><i class="fa fa-check"></i><b>1.10</b> Case 1: Variance known, sample size known</a></li>
<li class="chapter" data-level="1.11" data-path=""><a href="#case-2-variance-known-guarantee-stated-solving-for-sample-size"><i class="fa fa-check"></i><b>1.11</b> Case 2: Variance known, guarantee stated, solving for sample size</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Weak Law of Large Numbers</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">The Weak Law of Large Numbers</h1>
</div>
<div id="weak-law-of-large-numbers" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Weak Law of Large Numbers</h1>
<div id="set-up-the-space" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Set Up the Space</h2>
<p>Suppose that you have a coin that you are tossing that is unfair. But only a <em>little</em> bit unfair: the probability the coin comes up heads is 60%, while the probability that it comes up tails is 40%.</p>
<p>Does this mean that in any given flip you will get a 0.6 or 0.4? No! Of course not, it is a probability statement over the outcomes, which are in <span class="math inline">\(\{0, 1\}\)</span>.</p>
<blockquote>
<p>For an important digression in your studies, you could think about election forecasts: in 2016 most models predicted that Hillary Clinton would defeat Donald Trump in the electoral college. A leading forecasting sight, fivethirtyeight.com put gave Trump a 29% chance of winning. A lot of people casually interpreted this as though the outcome was going to be 29% Trump and 71% Clinton, which they interpreted as an outcome that would then place Clinton into office. When the coin came up “Trump” there was a <em>lot</em> of conversation about how to interpret these model predictions, and how to talk about these model predictions to those who are not familiar with the mental gymnastics of probability theory.</p>
</blockquote>
<p>Returning to the task at hand: Does a <span class="math inline">\(P(H=0.60)\)</span> mean that in <span class="math inline">\(10\)</span> tosses you are guaranteed to get <span class="math inline">\(6\)</span> that are heads and <span class="math inline">\(4\)</span> that are tails? Still no! While this is possible – and, in fact, the most likely outcome – there are many other outcomes that could come up. In fact, the outcomes range from zero heads all the way to ten heads. In expectation you will generate 60% of the tosses coming up heads, but in any single 10-toss sequences, there is randomness in this process that might lead a different number of heads to be shown. In fact, if you defined <span class="math inline">\(H_{(10)}\)</span> to be the summary statistic that sums the number of heads in a ten-toss sample, then you could develop the probability statement over each of the values that the statistic can obtain – illustrating that every statistic is, itself, a random variable.</p>
<p>If you defined a slightly different statistic, <span class="math inline">\(\overline{H}_{(n)} \equiv \overline{H} = \frac{1}{n} \sum_{i=1}^{n} H_{i}\)</span>, where now <span class="math inline">\(H_{i}\)</span> is a random variable that takes the value <span class="math inline">\(1\)</span> if a coin comes up heads and a value <span class="math inline">\(0\)</span> if it comes up tails, we might think of this as the sample average of the number of heads that come up. (Like the book, we note that <span class="math inline">\(\overline{H}_{(n)}\)</span> is a statistic that has a certain number of samples, <span class="math inline">\(n\)</span>, but that the notation is cumbersome and simply drop the subscript <span class="math inline">\((n)\)</span>.)</p>
<p>So then, what do we mean when we say that there is a 60% chance that it will come up heads? When you toss a single coin, you know that it will <em>either</em> be a Head or a Tail; and that if you toss two coins, there are three possible outcomes: {Zero Heads, One Head, Two Heads}, and that if you toss ten coins there is still no guarantee that you will get exactly six heads.</p>
<p>But, you likely have a belief that as you toss more and more coins, the sample average should get closer to the <span class="math inline">\(0.6\)</span> population parameter. This is a good belief to have, because it is true! But, why is it true? What about probability makes this happen?</p>
<p>In the rest of this demo, we’re going to first establish that as we toss more and more coins, the sample average actually gets closer to the population parameter. Then, we’re going to reason about why (the answer is <em>Chebyshev’s Inequality for the Sample Average</em>), and then we’ll reason about whether this also works for other distributions than coin tosses.</p>
</div>
<div id="coin-tosses" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Coin Tosses</h2>
<p>Let’s define a function that is called <code>toss_coin</code> that represents a coin that actually has an expected value of coming up heads that is 0.6. (This is little more than writing a named function that sets values for the already existing <code>rbinom</code> function. In general, this is bad coding practice to overload an already existing function, but it is useful for <em>teaching</em> this particular concept, because we can write <code>toss_coin</code> rather than <code>rbinom</code>.)</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>toss_coin <span class="ot">&lt;-</span> <span class="cf">function</span>(times, coins, prob_of_heads) { </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rbinom</span>(<span class="at">n =</span> times, <span class="at">size =</span> coins, <span class="at">prob =</span> prob_of_heads)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  }</span></code></pre></div>
<p>Then, we can sample from this coin’s distribution by tossing it a small number of times.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">toss_coin</span>(<span class="at">times=</span><span class="dv">1</span>, <span class="at">coins=</span><span class="dv">1</span>, <span class="at">prob_of_heads =</span> <span class="fl">0.6</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">toss_coin</span>(<span class="at">times=</span><span class="dv">1</span>, <span class="at">coins=</span><span class="dv">1</span>, <span class="at">prob_of_heads =</span> <span class="fl">0.6</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="fu">toss_coin</span>(<span class="at">times=</span><span class="dv">1</span>, <span class="at">coins=</span><span class="dv">1</span>, <span class="at">prob_of_heads =</span> <span class="fl">0.6</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="fu">toss_coin</span>(<span class="at">times=</span><span class="dv">1</span>, <span class="at">coins=</span><span class="dv">1</span>, <span class="at">prob_of_heads =</span> <span class="fl">0.6</span>)</span></code></pre></div>
<pre><code>## [1] 1
## [1] 1
## [1] 1
## [1] 0</code></pre>
<p>Suppose that we wanted to toss more than one coin? It would, obviously get pretty cumbersome if we had to issue this <code>toss_coin(times=1, coins=1)</code> command say, 1000 times. Lucky for us, we’ve written the function so that it generalizes to more coins! To toss ten coins, we can call for <code>toss_coin(times=1, coins=10)</code>, and for a million, <code>toss_coin(times=1, coins=1e6)</code>.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">toss_coin</span>(<span class="at">times=</span><span class="dv">1</span>, <span class="at">coins=</span><span class="dv">10</span>, <span class="at">prob_of_heads=</span><span class="fl">0.6</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">toss_coin</span>(<span class="at">times=</span><span class="dv">10</span>, <span class="at">coins=</span><span class="dv">1</span>, <span class="at">prob_of_heads=</span><span class="fl">0.6</span>)</span></code></pre></div>
<pre><code>## [1] 5
##  [1] 1 1 0 0 0 1 0 1 0 0</code></pre>
<p>Notice that if we increase the number of <code>times</code> it is like tossing the same coin a larger number of times; and if we increase the number of coins, it is like increaseing the number of coins that we are tossing at any single time. These actually have the same interpretation, since all the tosses are independent of all the other tosses, but it <em>does</em> change the way the data is represented to us.</p>
</div>
<div id="tossing-more-coins" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Tossing more coins</h2>
<p>Imagine undertaking the following process:</p>
<pre><code>1. Pick up a single coin, toss it, and record the average of the toss; then, 
2. Pick up another coin, toss both, and record the average of the two tosses; then, 
3. Pick up another coin, toss all three, and record the average of the three tosses; then ... 
... 
1000. Pick up another coin, toss all thousand and record the average of the thousand tosses. </code></pre>
<p>What would the sample average data look like from this sort of <em>simulation</em>.</p>
<p>To create the data, do the following</p>
<ol style="list-style-type: decimal">
<li>First, make a dataframe that has 1,000 rows. Let the first column, <code>coins</code> index the number of coins that were tossed in that simulation. Let the second column <code>heads</code> report the number of heads that came up. And, let the third column <code>average</code> report the number of heads as a proportion of the number of tosses.</li>
</ol>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>toss_many_coins <span class="ot">&lt;-</span> <span class="cf">function</span>(times, coins, prob_of_heads) { </span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>  toss_data_ <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">id=</span><span class="dv">1</span><span class="sc">:</span>coins) <span class="sc">%&gt;%</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>      <span class="at">coins =</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">n</span>(), </span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>      <span class="at">heads =</span> <span class="fu">toss_coin</span>(<span class="at">times=</span><span class="dv">1</span><span class="sc">:</span><span class="fu">n</span>(), <span class="at">coins=</span>coins, <span class="at">prob_of_heads=</span>prob_of_heads), </span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>      <span class="at">average =</span> heads <span class="sc">/</span> coins)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>toss_data <span class="ot">&lt;-</span> <span class="fu">toss_many_coins</span>(<span class="at">times=</span><span class="dv">1</span>, <span class="at">coins=</span><span class="dv">1000</span>, <span class="at">prob_of_heads=</span><span class="fl">0.6</span>)</span></code></pre></div>
<p>Notice a few neat things about this process of creating data, using <code>dplyr</code>. First, we’ve built the dataframe to have 1,000 rows, indexed by a column that we have called <code>id</code>. Unlike numpy over in python, this <code>id</code> column doesn’t have any special properties, and we could have named it anything. Second, within the <code>mutate</code> call, we’re making new data. One surprising behavior in this call is that you’re able to create new columns, predicated on the existence of other columns, that are made at the same time.</p>
<p>Also, notice that within the function I called the object that is going to be returned <code>toss_data_</code> with an <code>_</code> at the end. This doesn’t <em>do</em> anything, per se, but it does help to distinguish between the results <em>internal</em> to the function, and what we’re going to return out to the global namespace. Essentially, this keeps us from confusing things as humans, which means that the computer will also not be confused.</p>
<p>This isn’t a <em>pure</em> inception event, because the newly created variables need to be ordered chronologically within the mutate call. This means that the following code would not work:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">id=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span>) <span class="sc">%&gt;%</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">heads =</span> <span class="fu">toss_coin</span>(<span class="at">times=</span>tosses),</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">tosses =</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">n</span>(),</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">average =</span> heads <span class="sc">/</span> tosses)</span></code></pre></div>
<p>We can view the first six rows of this data by calling for the <code>head()</code> of the dataframe, and see that things seem sensible.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(toss_data)</span></code></pre></div>
<pre><code>##   id coins heads   average
## 1  1     1     0 0.0000000
## 2  2     2     1 0.5000000
## 3  3     3     2 0.6666667
## 4  4     4     4 1.0000000
## 5  5     5     3 0.6000000
## 6  6     6     6 1.0000000</code></pre>
<p>But, simply looking at the rows of data isn’t presenting this data in its most compelling way. Instead, this is absolutely the place for a plot.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>toss_data <span class="sc">%&gt;%</span> </span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span> </span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x=</span>coins, <span class="at">y=</span>average) <span class="sc">+</span> </span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">color =</span> berkeley_blue, <span class="at">size =</span> <span class="fl">0.1</span>) <span class="sc">+</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y=</span><span class="fl">0.6</span>), <span class="at">color =</span> california_gold) <span class="sc">+</span> </span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lims</span>(<span class="at">y =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&#39;Coin toss averages vs. sample size&#39;</span>, </span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">subtitle =</span> <span class="st">&#39;As sample size increases, sample averages are closer to 0.6&#39;</span>,</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&#39;Number of tosses&#39;</span>, </span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&#39;Average number of heads&#39;</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<p><img src="unit_05_weak-law-of-large-numbers_files/figure-html/plot%20the%20toss%20data-1.png" width="672" /></p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>coin_plot <span class="ot">&lt;-</span> <span class="cf">function</span>(times, coins, prob_of_heads) { </span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>  coin_plot_ <span class="ot">&lt;-</span> <span class="fu">toss_many_coins</span>(<span class="at">times=</span>times, <span class="at">coins=</span>coins, <span class="at">prob_of_heads=</span>prob_of_heads) <span class="sc">%&gt;%</span> </span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>() <span class="sc">+</span> </span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">x=</span>coins, <span class="at">y=</span>average) <span class="sc">+</span> </span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">color =</span> berkeley_blue, <span class="at">size =</span> <span class="fl">0.1</span>) <span class="sc">+</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y=</span>prob_of_heads), <span class="at">color =</span> california_gold) <span class="sc">+</span> </span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lims</span>(<span class="at">y =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme</span>(</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>      <span class="at">plot.background =</span> <span class="fu">element_rect</span>(<span class="at">fill =</span> <span class="st">&#39;transparent&#39;</span>, <span class="at">color =</span> <span class="cn">NA</span>), </span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>      <span class="at">panel.border =</span> <span class="fu">element_blank</span>()) <span class="sc">+</span> </span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>      <span class="at">title =</span> <span class="st">&#39;Coin toss averages vs. sample size&#39;</span>, </span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>      <span class="at">subtitle =</span> <span class="fu">sprintf</span>(<span class="st">&#39;As sample size increases, sample averages are closer to %.2f&#39;</span>, prob_of_heads),</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>      <span class="at">x =</span> <span class="st">&#39;Number of tosses&#39;</span>, </span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>      <span class="at">y =</span> <span class="st">&#39;Average number of heads&#39;</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>plot_0<span class="fl">.6</span> <span class="ot">&lt;-</span> <span class="fu">coin_plot</span>(<span class="at">times=</span><span class="dv">1</span>, <span class="at">coins=</span><span class="dv">1000</span>, <span class="at">prob_of_heads=</span><span class="dv">1</span>)</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>plot_0<span class="fl">.6</span></span></code></pre></div>
<p><img src="unit_05_weak-law-of-large-numbers_files/figure-html/plot%20the%20toss%20data-2.png" width="672" /></p>
<p>This seems to be doing what we want it to do! What do you think will happen if the probability of success is much higher?</p>
<ul>
<li>What will this look like if the probability of heads is <span class="math inline">\(1.0\)</span>?</li>
<li>What will this look like if the probability of heads is <span class="math inline">\(0.9\)</span>?</li>
<li>What will this look like if the probability of heads is <span class="math inline">\(0.2\)</span>?</li>
</ul>
<p>We will plot these on the next page.</p>
</div>
<div id="plot-distinct-distributions" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> Plot distinct distributions</h2>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>plot_1<span class="fl">.0</span> <span class="ot">&lt;-</span> <span class="fu">coin_plot</span>(<span class="at">times=</span><span class="dv">1</span>, <span class="at">coins=</span><span class="dv">1000</span>, <span class="at">prob_of_heads=</span><span class="fl">1.0</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>plot_0<span class="fl">.9</span> <span class="ot">&lt;-</span> <span class="fu">coin_plot</span>(<span class="at">times=</span><span class="dv">1</span>, <span class="at">coins=</span><span class="dv">1000</span>, <span class="at">prob_of_heads=</span><span class="fl">0.9</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>plot_0<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">coin_plot</span>(<span class="at">times=</span><span class="dv">1</span>, <span class="at">coins=</span><span class="dv">1000</span>, <span class="at">prob_of_heads=</span><span class="fl">0.2</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>plot_1<span class="fl">.0</span> <span class="sc">/</span> plot_0<span class="fl">.9</span> <span class="sc">/</span> plot_0<span class="fl">.2</span> <span class="sc">+</span> </span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot_layout</span>(<span class="at">guides =</span> <span class="st">&#39;collect&#39;</span>)</span></code></pre></div>
<p><img src="unit_05_weak-law-of-large-numbers_files/figure-html/many%20plots-1.png" width="672" /></p>
<p>What do you notice about the rate of convergence in probability between the different plots? Do some of them seem to be moving toward the true population expectation faster than others? Why is this? What is the variance of a Bernoulli random variable with <span class="math inline">\(P(H) = 0.9\)</span>, compared to one with <span class="math inline">\(P(H) = 0.6\)</span>?</p>
</div>
<div id="weak-law-of-large-numbers-1" class="section level2" number="1.5">
<h2><span class="header-section-number">1.5</span> Weak Law of Large Numbers</h2>
<p>What we are illustrating above is the <strong>Weak Law of Large Numbers</strong> (WLLN), which says that as we take more samples, the sample average, <span class="math inline">\(\overline{X}\)</span>, will converge in the probability limit to the expected value of <span class="math inline">\(X\)</span>, <span class="math inline">\(E[X]\)</span>.</p>
<p>To use the formal definition:</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-1" class="theorem"><strong>Theorem 1.1  (Weak Law of Large Numbers) </strong></span>Define <span class="math inline">\(\overline{X}\)</span> to be <span class="math inline">\(\frac{1}{n} \sum_{i=1}^{n} X_{i}\)</span>.</p>
<p>Then, if <span class="math inline">\(X_1, \dots, X_n\)</span> are i.i.d. random variables with finite, but positive variance <span class="math inline">\(0 &lt; V[X] &lt; \infty\)</span>,</p>
<p><span class="math display">\[
  \overline{X}_{(n)} \overset{p}\rightarrow E[X]
\]</span></p>
</div>
<p>This means that, even though we never <em>really</em> get to know the population parameter that is the <span class="math inline">\(E[X]\)</span>, if we take a lot of draws and take the average of those draws, it become increasingly close.</p>
<p>This is <em>very</em> useful! It means that we can <em>know</em> something that is <em>fundamentally</em> unknowable if we have enough data.</p>
</div>
<div id="the-wlln-is-about-a-probability-limit." class="section level2" number="1.6">
<h2><span class="header-section-number">1.6</span> The WLLN is about a probability limit.</h2>
<p>It makes the statement that, as we add data to a sample average estimator, that estimator will get closer and closer to the population expectation.</p>
<p>But, what provides this guarantee? Why is it that the WLLN <em>happens</em>? To answer this question requires that we talk about Chebyshev’s Inequality.</p>
</div>
<div id="chebyshevs-inequality" class="section level2" number="1.7">
<h2><span class="header-section-number">1.7</span> Chebyshev’s Inequality</h2>
<p>Earlier in our reading we read <em>Theorem 2.1.18</em> which says:</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-2" class="theorem"><strong>Theorem 1.2  (Chebyshev's Inequality) </strong></span>Let <span class="math inline">\(X\)</span> be a random varaible with finite <span class="math inline">\(\sigma[X] &gt; 0\)</span>. Then, &gt; 0,</p>
<p><span class="math display">\[
  Pr\big[\left|X - E[X] \right| \geq \epsilon\sigma[X]\big] \leq \frac{1}{\epsilon^2}
\]</span></p>
</div>
<p>The book shows in the proof for <em>Theorem 3.2.5</em> how to recast this inequality for a single draw, into an inequality for the sample average. But, it does the math <strong>really</strong> quickly, so spread it out some here.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-3" class="theorem"><strong>Theorem 1.3  (Markov Inequality) </strong></span><span class="math display">\[
  Pr[X \geq c\cdot a] \leq \frac{E[X]}{c\cdot a}
\]</span></p>
<p>where <span class="math inline">\(X\)</span> is a random variable, and <span class="math inline">\(c\)</span> and <span class="math inline">\(a\)</span> are both positive constants.</p>
</div>
<div class="proposition">
<p><span id="prp:pfd" class="proposition"><strong>Proposition 1.1  (Inequalities for Days) </strong></span>The part that it took Alex a while to appreciate is how eliminating <span class="math inline">\(c\)</span> works. Suppose that you wanted to eliminate <span class="math inline">\(c\)</span> from both sides of this <em>outer</em> inequality – the one that separates the probability statement from the ratio of expectations.</p>
<p>Written this way we can see that to remove <span class="math inline">\(c\)</span> from the left side of the inequality we would divide by <span class="math inline">\(c\)</span>; to remove <span class="math inline">\(c\)</span> from the right hand side, we would need to multiply by <span class="math inline">\(c\)</span>. This could be surprising, because it is opposite to what you might have a heuristic for doing. We need to treat the constant this way because it is nested within another inequality, in the probability statement.</p>
<p>If you reason about it for a moment, this behavior is reasonable.</p>
<p>Suppose that you have some random variable, and you ask for the probability that it is larger than some value. If that value is larger (i.e. <span class="math inline">\(c\)</span> is positive and larger than one), then the probability should be smaller. If that v alue is smaller (i.e. <span class="math inline">\(c\)</span> is positive, but smaller than one) then the probabliy should be larger.</p>
<p>To get closer to how we will use this fact in a moment, note that we could rewrite the statement as,</p>
<p><span class="math display">\[
\left\{Pr[X \geq c \sigma] \leq \frac{E[X]}{c\sigma}\right\} \Leftrightarrow \left\{Pr[X \geq \sigma] \leq \frac{E[X]}{\sigma} \right\}
\]</span></p>
</div>
</div>
<div id="chevyshev-inequlaity-for-the-sample-average" class="section level2" number="1.8">
<h2><span class="header-section-number">1.8</span> Chevyshev Inequlaity for the Sample Average</h2>
<p>The sample average, <span class="math inline">\(\overline{X}\)</span>, is a random variable – it is simply a summarising function of <span class="math inline">\(X\)</span> which is itself a random variable. Because of this, we can rewrite the classic statement of the Chevyshev inequality, just with a different (but known) statement of its sampling variance.</p>
<p>Note, that the sampling variance for <span class="math inline">\(\overline{X}\)</span>, <span class="math inline">\(V[\overline{X}] = \frac{V[X]}{n}\)</span>, which was proven just before the statement of Chebyshev’s Inequality for the Sample Mean in the book. Then, using the definition and notation for standard deviation, we can write:</p>
<p><span class="math display">\[
  \begin{aligned}
      Pr\big[\left|X - E[X] \right| \geq \epsilon\sigma[X]  \big] &amp; \leq \frac{1}{\epsilon^2} \\
      Pr\big[\left|\overline{X} - E[X] \right| \geq \epsilon  \sigma[\overline{X}]\big] &amp; \leq \frac{1}{\epsilon^2} &amp; \text{(Changing }X \rightarrow \overline{X}) \\
      Pr\big[\left|\overline{X} - E[X] \right| \geq \epsilon\big] &amp; \leq \frac{\sigma[\overline{X}]}{\epsilon^2} &amp;\text{(Using Prop 1.1)}
  \end{aligned}
\]</span></p>
<p>To complete the statement, we can use the theorem for <span class="math inline">\(\sigma[\overline{X}]\)</span>,</p>
<p><span class="math display">\[
  \begin{aligned} 
    Pr\big[\left|\overline{X} - E[X] \right| \geq \epsilon\big] \leq \frac{V[X]}{\epsilon^{2}n}
  \end{aligned} 
\]</span></p>
</div>
<div id="using-chebyshevs-inequality-for-the-sample-mean" class="section level2" number="1.9">
<h2><span class="header-section-number">1.9</span> Using Chebyshev’s Inequality for the Sample Mean</h2>
<p>Chebyshev’s gives us a way to reason about the probably that the sample average will be more than some <strong>particular</strong> <span class="math inline">\(\epsilon\)</span> away from the true expectation.</p>
<p>Specifically, suppose that we want to know:</p>
<blockquote>
<p>For the particular coin that we’re tossing with a probability of landing heads = 60%, what is the probability that the sample mean is more than 0.1 away from the true expected value, given some number of tosses we have conducted, <span class="math inline">\(n\)</span>?</p>
</blockquote>
<p>That is basically asking the following:</p>
<p><span class="math display">\[
P\left[|\overline{X} - E[X] | \geq 0.1 \right] \leq \frac{V[X]}{0.1^2n}
\]</span>
In this particular case, we can know the value for <span class="math inline">\(V[X]\)</span> of our coin:</p>
<p><span class="math display">\[
\begin{align*}
  V[X] &amp;= E[X^2] - E[X]^2 \\ 
       &amp;= E[X^2] - (0.6)^2 \\ 
       &amp;= E[X^2] - 0.36 \\ 
       &amp;= \sum_{\forall x} x^2 \cdot f_x - 0.36 \\ 
       &amp;= (0^2 \cdot 0.4 + 1^2 \cdot 0.6) - 0.36 \\ 
       &amp;= (0 + 0.6) - 0.36 \\ 
  V[X] &amp;= 0.24
\end{align*}
\]</span></p>
<p>And so, we can substitute this into our Chebyshev equation and do a little evaluation of the math</p>
<p><span class="math display">\[
P\left[|\overline{X} - E[X] | \geq 0.01 \right] \leq \frac{.24}{0.01n}
\]</span></p>
<p>There are a number of ways that you <em>could</em> use Chevyshev’s to produce a guarantee.</p>
<ul>
<li>Notice that this guarantee will be <strong>agnostic</strong> to the actual probability distribution of the random variable – this means that it will hold for any random variable that has a finite variance.<br />
</li>
<li>If you have additional information about the random variable, specifically information about how it is shaped in the form of a specific formula that describes its probability distribution, you <em>could</em> do better than Chevyshev’s.</li>
<li>But, Chebyshev’s provides a useful bound that applies is all circumstances.</li>
</ul>
<p>Notice that this inequality <strong>must</strong> hold; it isn’t that you have to tinker with parameters in it to ensure that it holds. No, instead, this is an iron-clad rule. And so, if you posses information that locks down one side of the inequality, you can know that the other side will be bound by the inequality.</p>
<ol style="list-style-type: decimal">
<li>If you know <span class="math inline">\(V[X]\)</span>, and the number of draws that you have taken, <span class="math inline">\(n\)</span>, then you can know the Chebyshev guarantees that are implied for some <span class="math inline">\(\epsilon\)</span>.</li>
<li>If you know the Chebyshev guarantees that you would like to have, and <span class="math inline">\(V[X]\)</span>, then you can know the number of samples that will be required to produce sample averages that are closer than <span class="math inline">\(\epsilon\)</span> to the <span class="math inline">\(E[X]\)</span>.</li>
</ol>
</div>
<div id="case-1-variance-known-sample-size-known" class="section level2" number="1.10">
<h2><span class="header-section-number">1.10</span> Case 1: Variance known, sample size known</h2>
<p>In this case of a coin toss, we’ve already shown that the variance of this random variable is <span class="math inline">\(0.24\)</span>. Suppose that we had tossed the coin 10 times, and we wanted to know, “What are the Chebyshev guarantees for the sample mean that apply to this die?”</p>
<p>We can write the inequalty for the sample mean, substituting in the known values.</p>
<p><span class="math display">\[
  \begin{aligned} 
  P\left[|\overline{X} - E[X] | \geq \epsilon \right] &amp; \leq \frac{0.24}{\epsilon^2(10)} \\ 
    &amp; \leq \frac{0.024}{\epsilon^2} &amp; (\text{Divide by 10})
  \end{aligned}
\]</span></p>
<p><img src="unit_05_weak-law-of-large-numbers_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Reason about this plot:</p>
<ol style="list-style-type: decimal">
<li>If you take only <strong>one sample</strong>, and return the sample average, it is just the same as drawing a single data point. Notice that this produces the same result as <em>Theorem 2.1.18</em> in <em>Foundations of Agnostic Statistics</em>.
<ol style="list-style-type: decimal">
<li>Read where <span class="math inline">\(\epsilon = 1\)</span>. The Chebyshev guarantee for any random variable with <span class="math inline">\(V[X] = 0.24\)</span> says that there is about a <span class="math inline">\(25\%\)</span> chance of drawing a value that is more than one unit away from <span class="math inline">\(E[X] = 0.6\)</span>. In the case of this particular random variable, which is a coin toss, notice that the Chebyshev bounds are looser than what is actually possible. Because this is a coin, if we take a sample average of a single toss, we will have either a zero or a 1, which means that <span class="math inline">\(|\overline{X} - E[X]| = \{0.4, 0.6\}\)</span>It is not possible to draw a single value that is smaller than <span class="math inline">\(-0.4\)</span> or larger than <span class="math inline">\(1.6\)</span> because this random variable is bounded on <span class="math inline">\([0, 1]\)</span>. But, we only know that because we know more about the distribution of the random variable, something that Chebyshev doesn’t have (or require).</li>
<li>Read where <span class="math inline">\(\epsilon = 0.5\)</span>. The Chebyshev guarantee at this point is somewhat more sensible, and says that there is just less than a <span class="math inline">\(100\%\)</span> chance of drawing a value that is more than <span class="math inline">\(0.5\)</span> units away from <span class="math inline">\(E[X] = 0.6\)</span>. This is true, but again, we know more than the Chebyshev guarantee – namely that the random variable is bounded on <span class="math inline">\([0,1]\)</span>; with this additional knowledge, we could know that we can place this <span class="math inline">\(100\%\)</span> guarantee at <span class="math inline">\(\epsilon = 0.6\)</span>, not <span class="math inline">\(\epsilon = 0.5\)</span>.</li>
</ol></li>
<li>If you take <strong>one-thousand samples</strong> then we get much stronger guarantees. Essentially, Chebyshev’s provides a guarantee that the probability of generating a sample average that is more than <span class="math inline">\(0.15\)</span> units from 0.6 is <span class="math inline">\(15\%\)</span>. Because we know about the distribution, we can actually calculate the probability of drawing <span class="math inline">\(\overline{X}_{(1000)} &gt; 0.64\)</span> or <span class="math inline">\(\overline{X}_{(1000)} &lt; 0.56\)</span>. Or, we can simulate it!</li>
</ol>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>one_thousand_samples <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="at">n=</span><span class="dv">100000</span>, <span class="at">size=</span><span class="dv">1000</span>, <span class="at">prob=</span><span class="fl">0.6</span>) <span class="sc">/</span> <span class="dv">1000</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>probs <span class="ot">&lt;-</span> <span class="fu">mean</span>(one_thousand_samples <span class="sc">&lt;=</span> <span class="fl">0.56</span>) <span class="sc">+</span> <span class="fu">mean</span>(one_thousand_samples <span class="sc">&gt;=</span> <span class="fl">0.64</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>probs</span></code></pre></div>
<pre><code>## [1] 0.01056</code></pre>
<p>We interpret this in the following way:</p>
<ul>
<li>The Chebyshev Inequality for the sample average says that the probability of getting a sample average that is more than <span class="math inline">\(0.04\)</span> away from the true population expectation of <span class="math inline">\(0.60\)</span> is <span class="math inline">\(0.15\)</span>, or <span class="math inline">\(15\%\)</span>. This is a nice guarantee to have, and one that exists no matter the distribution of the random variable.</li>
<li>But, since we actually <em>do</em> know the variance of the random variable, we we can compute the actual bound. When we do this (in this case, using an approximation from a simulation), we find that only about <span class="math inline">\(1\%\)</span> fall outside this range.</li>
</ul>
<p>Why then is this useful? This is the last refresher about this proof strategy:</p>
<p>This is an <em>agnostic</em> approach that does not require us to know anything about the random variable. Instead, any random variable that has finite variance will have sample averages that converge in probability to the population expectation. This is the **Weak Law of Large Numbers*! Neat.</p>
</div>
<div id="case-2-variance-known-guarantee-stated-solving-for-sample-size" class="section level2" number="1.11">
<h2><span class="header-section-number">1.11</span> Case 2: Variance known, guarantee stated, solving for sample size</h2>
<p>If you know the variance of the random variable, then you can use Chevyshev’s inequality to determine the amount of data that you require in order to produce a sample average that has a known probability of being closer than <span class="math inline">\(\epsilon\)</span> to the true population expectation.</p>
<p>Suppose that we were still working with the coin with a <span class="math inline">\(0.6\)</span> probability of coming up heads that we have used throughout this demo. Then, as an example, you could ask, “How many tosses would I need to have the probability of getting a sample average that is closer than <span class="math inline">\(0.2\)</span> from the true population expectation to be <span class="math inline">\(80\%\)</span>? This will require just a little bit of re-writing, but it isn’t <em>too</em> hard (although, Alex will acknowledge that it took him 20 minutes to figure out where he was going to make this work).</p>
<p>Rearranging terms using valid statements:</p>
<p><span class="math display">\[
  \begin{aligned}
        P\left[|\overline{X} - E[X] | \geq \epsilon \right] &amp;=   \frac{V[X]}{\epsilon^2 n} &amp; (\text{Chebyshev&#39;s}) \\
        P\left[|\overline{X} - E[X] | \geq \epsilon \right] &amp;= 1 - P\left[|\overline{X} - E[X] | \leq \epsilon \right] &amp; (\text{Law of Total Probability}) \\ 
    1 - P\left[|\overline{X} - E[X] | \leq \epsilon \right] &amp;= \frac{V[X]}{\epsilon^2 n} &amp; (\text{Substitution}) \\ 
    - P\left[|\overline{X} - E[X] | \leq \epsilon \right] &amp;= \frac{V[X]}{\epsilon^2 n} - 1 \\ 
    P\left[|\overline{X} - E[X] | \leq \epsilon \right] &amp;= 1 - \frac{V[X]}{\epsilon^2 n} \\ 
  \end{aligned}
\]</span></p>
<p>This fact helps us move forward toward an answer! If we solve for <span class="math inline">\(n\)</span>, we are there!</p>
<p><span class="math display">\[
  \begin{aligned}
    0.80 &amp;= 1 - \frac{V[X]}{\epsilon^{2}n} \\ 
      &amp;= 1 - \frac{0.24}{(0.2^2)} \cdot \frac{1}{n} = 1 - \frac{0.24}{0.04} \cdot \frac{1}{n} \\ 
      &amp;= 1 - \frac{6}{n} \\ 
    0.80  &amp;= \frac{n - 6}{n} \\ 
    0.80 \cdot n  &amp;= n - 6 \\ 
    0.8n - n &amp;= -6 \\ 
    -0.2n &amp;= -6 \\
    n &amp;= 30
  \end{aligned}
\]</span></p>
<p>And so, Chebyshev says that if we draw 30 samples, the probability of generating a sample average that is larger than <span class="math inline">\(0.8\)</span>, or smaller than <span class="math inline">\(0.4\)</span> is <span class="math inline">\(0.2\)</span>.</p>
<p>To evaluate this guarantee, again simulate a series of draws with eight coins being tossed.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>thirty_tosses <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="at">n=</span><span class="dv">10000</span>, <span class="at">size =</span> <span class="dv">30</span>, <span class="at">prob =</span> <span class="fl">0.6</span>) <span class="sc">/</span> <span class="dv">30</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(thirty_tosses <span class="sc">&lt;</span> <span class="fl">0.8</span>) <span class="sc">-</span> <span class="fu">mean</span>(thirty_tosses <span class="sc">&lt;</span> <span class="fl">0.4</span>)</span></code></pre></div>
<pre><code>## [1] 0.974</code></pre>
<p>We see that Chebyshev is actually requiring us to take more samples than are actually necessary, if we knew the actual Bernoulli distribution that underlies the data generating process. Indeed, after a little bit of trial and error, I was able to figure out that about nine samples is enough data to provide this guarantee.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>nine_tosses <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="at">n=</span><span class="dv">10000</span>, <span class="at">size =</span> <span class="dv">9</span>, <span class="at">prob =</span> <span class="fl">0.6</span>) <span class="sc">/</span> <span class="dv">9</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(nine_tosses <span class="sc">&lt;</span> <span class="fl">0.8</span>) <span class="sc">-</span> <span class="fu">mean</span>(nine_tosses <span class="sc">&lt;</span> <span class="fl">0.4</span>)</span></code></pre></div>
<pre><code>## [1] 0.8317</code></pre>
</div>
</div>
            </section>

          </div>
        </div>
      </div>


    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": false,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
