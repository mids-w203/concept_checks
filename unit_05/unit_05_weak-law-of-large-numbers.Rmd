---
title: "The Weak Law of Large Numbers"
output: 
  bookdown::gitbook
---
```{r, echo=FALSE, message=FALSE}
library(dplyr)
library(magrittr)

library(ggplot2)
library(patchwork)
theme_set(theme_minimal())
```

# Weak Law of Large Numbers 
## Set Up the Space

Suppose that you have a coin that you are tossing that is unfair. But only a *little* bit unfair: the probability the coin comes up heads is 60%, while the probability that it comes up tails is 40%. 

Does this mean that in any given flip you will get a 0.6 and 0.4? No! Of course not, it is a probability statement. 

Although, for an important digression in your studies, you could think about election forecasts: in 2016 most models predicted that Hillary Clinton would defeat Donald Trump in the electoral college. A leading forecasting sight, fivethirtyeight.com put gave Trump a 29% change of winning. A lot of people casually interpreted this as though the outcome was going to be 29% Trump and 71% Clinton. Then, when the coin came up "Trump" there was a *lot* of conversation about how to interpret these model predictions. 

Returning to the task at hand: Does a P(heads = 0.60) mean that in 10 tosses you will get 6 that are heads and 4 that are tails? Still no! In expectation you will generate 60% of the tosses coming up heads, but in any single 10-toss sequences, there is randomness in this process that might lead any number of heads to be shown. 

So then, what do we mean when we say that there is a 60% chance that it will come up heads? And, how could we come to know this? 

Let's define a function that is called `toss_coin` that represents a coin that actually has an expected value of coming up heads that is 0.6. (This is little more than writing a named function that sets values for the already existing `rbinom` function. In general, this is bad coding practice to overload an already existing function, but it is useful for *teaching* this particular concept.) 

```{r}
toss_coin <- function(times) { 
  rbinom(n = times, size = 1, prob = 0.6)
  }
```

Then, we can sample from this coin's distribution by tossing it a small number of times.

```{r}
toss_coin(1)
```

Or, we can sample from this coin's distribution by tossing it a large number of times. 

```{r}
toss_coin(100)
```

## Weak Law of Large Numbers 

The Weak Law of Large Numbers (WLLN) says that as we take more samples, the sample average, $\overline{X}$, will converge in the probability limit to the expected value of $X$, $E[X]$. 

To use the formal definition: 

> Define $\overline{X}$ to be $\frac{1}{n} \sum_{i=1}^{n} X_{i}$. 
> 
> Then, if $X_1, \dots, X_n$ are i.i.d. random variables with finite, but positive variance $0 < V[X] < \infty$, 
> 
$$
\overline{X}_{(n)} \overset{p}\rightarrow E[X]
$$ 

This means that, even though we never *really* get to know the population parameter that is the $E[X]$, if we take a lot of draws and take the average of those draws, it become increasingly close. 

This is *very* useful! 

It means that we can *know* something that is *fundimentally* unknowable if we have enough data.  

Let's show this. 

1. First, make a dataframe that has 1,000 rows. On each of those rows, toss a single coin. 
2. Then, create a new varaible called `cumulative_mean` that is the running average of the data series from the first row to the last row. This is sort of a short-cut so that we don't have to run a **lot** of coin tosses; and, in some ways this breaks the rules for the data generating process (since it is actually only *one* data generating process that we're sub-sampling from) but it gets the point across. 
3. Third, and finally, plot the cumulative mean on the y-axis and the number of rows that are being considered on the x-axis. (Notice that you are plotting the same data, in two ways, once with a non-transformed x-axis, the other with a transformed x-axis)

```{r make data}
number_of_rows <- 1000

d <- data.frame(
  n = 1:number_of_rows,
  x = toss_coin(number_of_rows)
)

d <- d %>%  
  mutate(
    cumulative_mean = cummean(x)
  )

plot_not_logged <- d %>%
  ggplot(aes(x = n, y = cumulative_mean)) + 
  geom_line() + 
  labs(
    x = 'Number of Tosses (not logged scale)', 
    y = 'Running Average'
  )

plot_logged <- d %>%
  ggplot(aes(x = n, y = cumulative_mean)) + 
  scale_x_continuous(trans = 'log2') + 
  geom_line() + 
  labs(
    x = 'Number of Tosses (logged scale)', 
    y = 'Running Average'
  )

plot_not_logged / plot_logged
```

The WLLN is about a probability limit, and so as we add more data we'll get ever closer. 

## Using Chebyshev's Inequality for the Sample Mean 

But we also know a specific bound for finite $n$.  This is what we call *Chebyshev's Inequality for the Sample Mean*. 

$$
P\left[|\overline{X} - E[X] | \geq \epsilon \right] \leq \frac{V[X]}{\epsilon^2n}
$$

Chebyshev's gives us a way to reason about the probably that the sample average will be more than some **particular** $\epsilon$ away from the true expectation. 

Specifically, suppose that we want to know:

> For the particular coin that we're tossing with a probability of landing heads = 60%, what is the probability that the sample mean is more than 0.01 away from the true expected value, given some number of tosses we have conducted, $n$?

Using Chebyshev's Inequality for the sample mean would proceed as follows: 

1. First, ask yourself the question, "What Chebyshev guarantee would I like to have?" 

> Would you like the probability that the distance between your sample average and the true expectation is larger than some $\epsilon$ to be 80%? 
> 
> Would you like this probability to be 90%? 
> 
> Would you like this probability to be 99%? 

This is the quantity that is useful on the left-hand side of Theorem 3.2.5. 

**Suppose for now that that you would like an 80% guarantee.** 

2. Then, you can use the rest of the theorem to solve for what value of $\epsilon$ provides that guarantee. 
The answer is that this probability will be less than 

$$
  0.80 = \frac{V[X]}{\epsilon^{2}n}
$$

In this particular case, we can know the value for $V[X]$ of our coin: 

$$
\begin{align*}
V[X] &= E[X^2] - E[X]^2 \\ 
     &= E[X^2] - (0.6)^2 \\ 
     &= E[X^2] - 0.36 \\ 
     &= \sum_{\forall x} x^2 \cdot f_x - 0.36 \\ 
     &= (0^2 \cdot 0.4 + 1^2 \cdot 0.6) - 0.36 \\ 
     &= (0 + 0.6) - 0.36 \\ 
V[X] &= 0.24
\end{align*}
$$

Or, since we've proven this in other work, if $X$ is a bernoulli rv, the $V[X] = p(1-p)$, 

$$
\begin{align*} 
  V[X] &= p(1-p) \\ 
       &= 0.6(1-0.6) \\ 
       &= 0.6 \cdot 0.4 \\ 
       &= 0.24
\end{align*} 
$$ 

We're seeking to solve this equation for $\epsilon$ on one side of the equation, and everything else on the other. 

Returning to our original statement, we said: 

$$
\begin{align*}
  0.80 &= \frac{V[X]}{\epsilon^2n} \\ 
  0.80 &= \frac{0.24}{\epsilon^2n} \\ 
  \epsilon^2 &= \frac{0.24}{0.80 \cdot n} \\ 
  \epsilon &= \sqrt{\frac{0.24}{0.80 \cdot n}}
\end{align*} 
$$ 

With this, we could plot what these bounds look like! Let's begin by making an object called `prob` that is our probability guarantee. 

Then, let's make a new variable on our data frame that is the epsilon size for each n. 

```{r}
prob <- 0.9

d <- d %>%  
  mutate(
    epsilon = sqrt(0.24 / (prob * n))
  )
```


tail(d)

### Basic, Exploratory Plot

Lets start to plot this with the key information that you want to communicate to yourself, how $\bar{X}$ changes as the number of samples increases. 

```{r}
d %>%  
  ggplot(aes(x = n, y = cumulative_mean)) + 
  scale_x_continuous(trans = 'log2') + 
  geom_line()
```

### Expository Plot

If you want to communicate this to others, you will have to take the time to place all of the context that you have in mind, into the plot. In this plot:

- We have placed the Chebyshev bounds in the plot 
- Kept the `cumulative_mean` line

But, notice that we have pulled the `aes(x = n, y = cumulative_mean)` out from the first `ggplot` call. This is because we do not want each of the plot elements to inherit this -- instead, we want the `geom_line` to take this, but the `geom_ribbon` to take a different set of y values. 

```{r}
cumulative_mean_not_logged <- d %>%  
  ggplot() + 
  geom_ribbon(
    aes(
      x = n, 
      ymin = .6 - epsilon, 
      ymax = .6 + epsilon), 
    fill = 'steelblue', alpha = 0.8) + 
  geom_line(aes(x = n, y = cumulative_mean), color = 'darkorange') + 
  geom_hline(yintercept = 0.6, color = 'steelblue') + 
  coord_cartesian(ylim = c(0,1)) + 
  labs(
    x = 'Number of Samples', 
    y = expression('Sample Average')) + 
  theme_minimal()

cumulative_mean_log_transformed <- cumulative_mean_not_logged + 
  scale_x_continuous(trans = 'log2')
  
cumulative_mean_not_logged | cumulative_mean_log_transformed  
```

Plotted in blue here is the distance from 0.6 that is necessary to produce an `r prob` guarantee. 

1. On the one hand, it this is a really great guarantee to have that makes *very, very, very* few assumptions about the data -- only that it has some finite variance. 
2. On the other hand, this is a really weak bound. Here's what that means: if we knew more about the data generating process, we could potentially use some **other** theorem that would give us a guarantee that was tigther to the truth, given the same number of observations. 

## Theoretical Implications 

Return back up a level from the coding to the thinking. 

> What are we guaranteed by the WLLN? 

So long as a random variable has finite variance, then as the number of samples from that random variable increases, the average of those samples will converge to the expected value of that random variable. 

This is really powerful! The only condition that we have placed on this statement is that the random variable have finite variance -- nothing else. 

Through Chebyshev's Inequality for the sample mean, we can characterize how quickly the sample averages will converge toward the expected value, namely at the rate of $\sqrt{\frac{V[X]}{n}}$. 
